{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and converting keras to tensorflow pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Feng\\Miniconda3\\envs\\aind-vui\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./model/keras_end_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'softmax/truediv:0' shape=(?, ?, 29) dtype=float32>]\n",
      "(?, ?, 13)\n"
     ]
    }
   ],
   "source": [
    "print(model.outputs)\n",
    "print(model.inputs[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 22 variables.\n",
      "Converted 22 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model\\\\model_end.pb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to ./model/tf_model.pb\n",
    "tf.train.write_graph(frozen_graph, \"model\", \"model_end.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "is_win = 'windows' in platform.platform().lower()\n",
    "\n",
    "# OpenVINO 2019\n",
    "if is_win:\n",
    "    mo_tf_path = '\"C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo_tf.py\"'\n",
    "else:\n",
    "    # mo_tf.py path in Linux\n",
    "    mo_tf_path = '/opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rnn_1_while_mul_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cogapp\n",
      "  Using cached cogapp-3.0.0-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: cogapp\n",
      "Successfully installed cogapp-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install cogapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python \"C:/Program Files (x86)/IntelSWTools/openvino/deployment_tools/tools/extension_generator/extgen.py\" new --mo-tf-ext --mo-op --ie-cpu-ext --ie-gpu-ext --output_dir=rnn_1_while_mul_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tC:\\Users\\Feng\\optimizekerasmodel\\./model/model_end.pb\n",
      "\t- Path for generated IR: \tC:\\Users\\Feng\\optimizekerasmodel\\./model\n",
      "\t- IR output name: \tmodel_end\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t(1,584,13)\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2019.3.0-408-gac8584cb7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Feng\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "[ ERROR ]  Something bad has happened with graph! Data node \"rnn_1/while/mul_1\" has 0 producers\n"
     ]
    }
   ],
   "source": [
    "input_shape = model.inputs[0].shape\n",
    "input_shape_str = str((1,584,13)).replace(' ','').replace('?', 'None')\n",
    "pb_file = './model/model_end.pb'\n",
    "output_dir = './model'\n",
    "\n",
    "!python {mo_tf_path} --input_model {pb_file} --output_dir {output_dir} --input_shape {input_shape_str} --data_type FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.2.0\n",
      "  Downloading tensorflow-1.2.0-cp36-cp36m-win_amd64.whl (21.2 MB)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\feng\\anaconda3\\lib\\site-packages (from tensorflow==1.2.0) (0.14.1)\n",
      "Collecting backports.weakref==1.0rc1\n",
      "  Using cached backports.weakref-1.0rc1-py3-none-any.whl (4.3 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\feng\\anaconda3\\lib\\site-packages (from tensorflow==1.2.0) (1.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\feng\\anaconda3\\lib\\site-packages (from tensorflow==1.2.0) (0.32.2)\n",
      "Collecting markdown==2.2.0\n",
      "  Using cached Markdown-2.2.0.tar.gz (236 kB)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\feng\\anaconda3\\lib\\site-packages (from tensorflow==1.2.0) (1.14.5)\n",
      "Requirement already satisfied: html5lib==0.9999999 in c:\\users\\feng\\anaconda3\\lib\\site-packages (from tensorflow==1.2.0) (0.9999999)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in c:\\users\\feng\\appdata\\roaming\\python\\python36\\site-packages (from tensorflow==1.2.0) (3.6.1)\n",
      "Requirement already satisfied: bleach==1.5.0 in c:\\users\\feng\\anaconda3\\lib\\site-packages (from tensorflow==1.2.0) (1.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\feng\\anaconda3\\lib\\site-packages (from protobuf>=3.2.0->tensorflow==1.2.0) (45.1.0)\n",
      "Building wheels for collected packages: markdown\n",
      "  Building wheel for markdown (setup.py): started\n",
      "  Building wheel for markdown (setup.py): finished with status 'done'\n",
      "  Created wheel for markdown: filename=Markdown-2.2.0-py3-none-any.whl size=136956 sha256=3e16abb42500c9d581fab7f929c1ab0f37e09a578f42e7174b96c882c81a8957\n",
      "  Stored in directory: c:\\users\\feng\\appdata\\local\\pip\\cache\\wheels\\ca\\3a\\29\\594713d45f4f387ff7ba5e713398406dc0fd4496eec8b470c0\n",
      "Successfully built markdown\n",
      "Installing collected packages: backports.weakref, markdown, tensorflow\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 2.6.11\n",
      "    Uninstalling Markdown-2.6.11:\n",
      "      Successfully uninstalled Markdown-2.6.11\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 1.7.1\n",
      "    Uninstalling tensorflow-1.7.1:\n",
      "      Successfully uninstalled tensorflow-1.7.1\n",
      "Successfully installed backports.weakref-1.0rc1 markdown-2.2.0 tensorflow-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unityagents 0.4.0 has requirement protobuf==3.5.2, but you'll have protobuf 3.6.1 which is incompatible.\n",
      "ERROR: unityagents 0.4.0 has requirement tensorflow==1.7.1, but you'll have tensorflow 1.2.0 which is incompatible.\n",
      "ERROR: tensorboard 1.7.0 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ ERROR ]  Path to input model or saved model dir is required: use --input_model, --saved_model_dir or --input_meta_graph\n"
     ]
    }
   ],
   "source": [
    "!python {mo_tf_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Openvino Optm",
   "language": "python",
   "name": "aind-vui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
